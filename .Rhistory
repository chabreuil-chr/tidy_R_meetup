# 1 lib import ====
library(tidyverse)
library(tidymodels)
# 2 import data ====
df = read_csv(file = here::here('data','aw_fb_data.csv'))
df = df[,-c(1:2)]
colnames(df)
df_aw = df %>% filter(device == 'apple watch')
df_fb = df %>% filter(device == 'fitbit')
set.seed(123)
data_split <- initial_split(df_aw, # updated data
strata = activity)
train_data <- training(data_split)
test_data <- testing(data_split)
df_rec = recipe(activity ~ . , data = train_data) %>%
step_num2factor(gender,
transform = function(x) x + 1,
levels = c('femme', 'homme')) %>%
step_rm(device) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric(), -gender_homme) %>%
step_zv(all_numeric()) %>%
step_corr(all_numeric_predictors(), threshold = 0.7)
encoded = df_rec %>%
prep() %>%
juice()
str(train_data)
str(encoded)
rf_spec <-
rand_forest(trees = 200) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
activity_wflow <-
workflow() %>%
add_model(rf_spec) %>%
add_recipe(df_rec)
activty_fit <-
activity_wflow %>%
fit(data = train_data)
predict(activty_fit, test_data)
activ_aug <-
augment(activty_fit, test_data) # prediction prob (au lieu de predict(type = 'pro'))
activ_aug %>%
select(activity, .pred_class, contains(".pred_"))
activ_aug %>%
select(-.pred_class) %>%
roc_curve(truth = as.factor(activity), contains(".pred_")) %>%
autoplot()
activ_aug %>%
select(-.pred_class) %>%
roc_auc(truth = as.factor(activity), contains(".pred_"))
activ_aug %>%
conf_mat(activity, .pred_class) %>%
autoplot(type = "heatmap")
activ_aug %>%
accuracy(truth = as.factor(activity), .pred_class)
set.seed(100)
cv_folds <-
vfold_cv(train_data,
v = 3,
strata = class)
xgb_spec <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification")
knn_spec <-
nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors
set_engine("kknn") %>%
set_mode("classification")
rf_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(rf_spec)
xgb_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec)
knn_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(knn_spec)
rf_res <-
rf_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
accuracy,roc_auc, sens, spec),
control = control_resamples(
save_pred = TRUE)
)
xgb_spec <-
xgb_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
accuracy,roc_auc, sens, spec),
control = control_resamples(
save_pred = TRUE)
)
knn_res <-
knn_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
accuracy,roc_auc, sens, spec),
control = control_resamples(
save_pred = TRUE)
)
set.seed(100)
cv_folds <-
vfold_cv(train_data,
v = 3,
strata = class)
str(train_data)
set.seed(100)
cv_folds <-
vfold_cv(train_data,
v = 3,
strata = activity)
rf_spec <-
rand_forest() %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
xgb_spec <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification")
knn_spec <-
nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors
set_engine("kknn") %>%
set_mode("classification")
rf_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(rf_spec)
xgb_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(xgb_spec)
knn_wflow <-
workflow() %>%
add_recipe(df_rec) %>%
add_model(knn_spec)
rf_res <-
rf_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
accuracy,roc_auc, sens, spec),
control = control_resamples(
save_pred = TRUE)
)
xgb_spec <-
xgb_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
accuracy,roc_auc, sens, spec),
control = control_resamples(
save_pred = TRUE)
)
knn_res <-
knn_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
accuracy,roc_auc, sens, spec),
control = control_resamples(
save_pred = TRUE)
)
rf_metrics <-
rf_res %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "Random Forest")
xgb_metrics <-
xgb_spec %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "XGB")
knn_metrics <-
knn_res %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "KNN")
model_compare <- bind_rows(
rf_metrics,
xgb_metrics,
knn_metrics
)
model_comp <-
model_compare %>%
select(model, .metric, mean, std_err) %>%
pivot_wider(names_from = .metric, values_from = c(mean, std_err))
model_comp %>%
arrange(mean_accuracy) %>%
mutate(model = fct_reorder(model, mean_accuracy)) %>% # order results
ggplot(aes(model, mean_accuracy, fill=model)) +
geom_col() +
coord_flip() +
scale_fill_brewer(palette = "Blues") +
geom_text(
size = 3,
aes(label = round(mean_accuracy, 1), y = mean_accuracy + 0.08),
vjust = 1
)
last_fit_rf <- last_fit(rf_wflow,
split = data_split,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec)
)
last_fit_rf %>%
collect_metrics()
# step_normalize(all_numeric(), -all_outcomes()) %>%
# step_dummy(all_nominal(), -all_outcomes()) %>%
# step_zv(all_numeric(), -all_outcomes()) %>%
# step_corr(all_numeric_predictors(), threshold = 0.8)
summary(df_rec) # comment on recoit le fichier origine
View(df_aw)
library(janitor)
library(rvest)
library(dplyr)
# Url
base_url = 'https://www.historique-meteo.net/france/ile-de-france/paris/'
paris_weather = data.frame()
# boucle de web scrap
for (y in 2010:2022){
for (m in 1:12){
message(paste0(y," - ", m))
if(m<10){
m = paste0('0',m)
}
my_url <-paste0(
base_url, y, '/', m, "/#jour"
)
month_weath <- my_url %>%
read_html() %>%
html_nodes('.col-lg-8') %>%
html_table()
table = as.data.frame(month_weath[1])
if("X6" %in% colnames(table)){
table_filter = table %>%
filter(is.na(X6) == T) %>%
mutate(date = lubridate::as_date(X1, format= '%d/%m/%Y'),
temp = X3) %>%
filter(is.na(date) == F) %>%
select(date, temp) %>%
mutate(temp_min = as.numeric(str_match(temp, "Températures : \\s*(.*?)\\s*°C/")[,2]),
temp_max = as.numeric(str_match(temp, "°C/\\s*(.*?)\\s*°CPrécipitations")[,2]),
temp = (temp_min + temp_max)/2)
}else{
table_filter = table %>%
filter(is.na(X5) == F) %>%
mutate(date = lubridate::as_date(X1, format= '%d/%m/%Y'),
temp = X3) %>%
filter(is.na(date) == F) %>%
select(date, temp) %>%
mutate(temp_min = as.numeric(str_match(temp, "Températures : \\s*(.*?)\\s*°C/")[,2]),
temp_max = as.numeric(str_match(temp, "°C/\\s*(.*?)\\s*°CPrécipitations")[,2]),
temp = (temp_min + temp_max)/2)
}
paris_weather = rbind(paris_weather, table_filter)
}
}
write.csv(paris_weather,here::here('data', 'paris_temp.csv'), row.names = FALSE)
View(paris_weather)
library(tidymodels)
## ── Attaching packages ─────────────────────────── tidymodels ## 0.1.4 ──
## ✔ broom        0.7.11     ✔ recipes      0.1.17
## ✔ dials        0.0.10     ✔ rsample      0.1.1
## ✔ dplyr        1.0.7      ✔ tibble       3.1.7
## ✔ ggplot2      3.3.6      ✔ tidyr        1.1.4
## ✔ infer        1.0.0      ✔ tune         0.1.6
## ✔ modeldata    0.1.1      ✔ workflows    0.2.4
## ✔ parsnip      0.1.7      ✔ workflowsets 0.1.0
## ✔ purrr        0.3.4      ✔ yardstick    0.0.9
## ── Conflicts ───────────────────────────────────────── ## tidymodels_conflicts() ──
## ✖ purrr::discard() masks scales::discard()
## ✖ dplyr::filter()  masks stats::filter()
## ✖ dplyr::lag()     masks stats::lag()
## ✖ recipes::step()  masks stats::step()
#| echo: false
library(tidyverse)
library(modeltime)
library(timetk)
library(lubridate)
library(ggblanket)
#| echo: false
# import des données
ts_data <- read_csv("data/paris_temp.csv") %>%
select(date, temp)
# format prophet
ts_data$ds = ts_data$date
ts_data$y = ts_data$temp
ts_data <- ts_data %>%
select(-c(date, temp))
# plot
ts_data %>%
plot_time_series(ds, y)
#| echo: false
knitr::kable(ts_data %>% tail(6))
?step_timeseries_signature
recipe_spec <- recipe(y ~ ds, training(split)) %>%
step_timeseries_signature(ds) %>%
step_rm(contains("am.pm"), contains("hour"), contains("minute"),
contains("second"), contains("xts")) %>%
step_fourier(ds, period = 365, K = 5) %>%
step_dummy(all_nominal())
#| echo: false
# import des données
ts_data <- read_csv("data/paris_temp.csv") %>%
select(date, temp)
# format prophet
ts_data$ds = ts_data$date
ts_data$y = ts_data$temp
ts_data <- ts_data %>%
select(-c(date, temp))
split = time_series_split(
ts_data,
assess = "15 days",
cumulative = TRUE
)
#| echo: false
split %>%
tk_time_series_cv_plan() %>%
filter(ds > "2022-03-01") %>%
plot_time_series_cv_plan(ds, y)
recipe_spec <- recipe(y ~ ds, training(split)) %>%
step_timeseries_signature(ds) %>%
step_rm(contains("am.pm"), contains("hour"), contains("minute"),
contains("second"), contains("xts")) %>%
step_fourier(ds, period = 365, K = 5) %>%
step_dummy(all_nominal())
table = recipe_spec %>% prep() %>% juice()
#| echo: false
knitr::kable(table %>% head(6))
ts_k_folds = rolling_origin(ts_data,
initial = 1600,
assess = 15,
skip = 120)
ts_k_folds  %>%
mutate(train = map(splits, analysis),
test = map(splits, assessment)) %>%
select(id, train, test) %>%
pivot_longer(-id) %>%
unnest(value) %>%
filter(id %in% c("Slice01", "Slice04", "Slice09")) %>%
gg_line(x = ds,
y = y,
col = name,
facet = id)
ts_k_folds  %>%
mutate(train = map(splits, analysis),
test = map(splits, assessment)) %>%
select(id, train, test) %>%
pivot_longer(-id) %>%
unnest(value) %>%
filter(id %in% c("Slice01", "Slice04", "Slice09"))
ts_k_folds
ts_k_folds  %>%
mutate(train = map(splits, analysis),
test = map(splits, assessment)) %>%
select(id, train, test) %>%
pivot_longer(-id) %>%
unnest(value) %>%
filter(id %in% c("Slice01", "Slice04", "Slice09")) %>%
slice(tail(row_number(), 100)) %>%
gg_line(x = ds,
y = y,
col = name,
facet = id)
ts_k_folds  %>%
mutate(train = map(splits, analysis),
test = map(splits, assessment)) %>%
select(id, train, test) %>%
pivot_longer(-id) %>%
unnest(value) %>%
filter(id %in% c("Slice01", "Slice04", "Slice09")) %>%
group_by(id) %>%
slice(tail(row_number(), 100)) %>%
gg_line(x = ds,
y = y,
col = name,
facet = id)
ts_k_folds  %>%
mutate(train = map(splits, analysis),
test = map(splits, assessment)) %>%
select(id, train, test) %>%
pivot_longer(-id) %>%
unnest(value) %>%
filter(id %in% c("Slice01", "Slice04", "Slice09")) %>%
group_by(id) %>%
slice(tail(row_number(), 100)) %>%
gg_line(x = ds,
y = y,
col = name,
facet = id,
facet_scales = 'free_x')
ts_k_folds  %>%
mutate(train = map(splits, analysis),
test = map(splits, assessment)) %>%
select(id, train, test) %>%
pivot_longer(-id) %>%
unnest(value) %>%
filter(id %in% c("Slice01", "Slice05", "Slice10", "Slice24")) %>%
group_by(id) %>%
slice(tail(row_number(), 100)) %>%
gg_line(x = ds,
y = y,
col = name,
facet = id,
facet_scales = 'free_x')
splits
splits = split
train_data = analysis(splits)
test_data = assessment(splits)
splits_calib <- initial_time_split(train_data, prop = 0.85)
model_spec_prophet_boost <- prophet_boost(
prior_scale_changepoints = 0.01,
prior_scale_seasonality = 5) %>%
set_engine("prophet_xgboost")
workflow_fit_prophet_boost <- workflow() %>%
add_recipe(recipe_spec) %>%
add_model(model_spec_prophet_boost) %>%
fit(training(splits_calib))
workflow_fit_prophet_boost <- workflow() %>%
add_recipe(recipe_spec) %>%
add_model(model_spec_prophet_boost) %>%
fit(training(splits_calib))
calib_table = workflow_fit_prophet_boost %>%
modeltime_calibrate(testing(splits_calib))
future_prophet_boost = calib_table %>%
modeltime_refit(train_data) %>%
modeltime_forecast(new_data = test_data,
actual_data = train_data)
future_prophet_boost
m_prophet = prophet::prophet(df = train_data,
seasonality.mode = 'additive',
changepoint.prior.scale = 0.01,
seasonality.prior.scale = 5)
future = prophet::make_future_dataframe(m_prophet, periods = nrow(test_data),
freq = 'day', include_history = FALSE)
predict(m_prophet, future) %>%
select(ds, yhat) %>%
mutate(type = 'prophet')
future_prophet_boost %>%
filter(.model_desc != 'ACTUAL') %>%
select(ds = .index, yhat = .value) %>%
mutate(type = 'prophet_xgb'))
future_prophet_boost %>%
filter(.model_desc != 'ACTUAL') %>%
select(ds = .index, yhat = .value) %>%
mutate(type = 'prophet_xgb')
