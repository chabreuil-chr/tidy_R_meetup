---
# title: "Présentation de TidyModels"
# subtitle: "R addict"
# author: "Clément Rieux"
# date: '22 Septembre 2022'
format: 
  revealjs:
    footer: "[https://github.com/clementrx/site_r](https://github.com/clementrx/site_r)&nbsp;&nbsp;&nbsp;"
    theme: [custom.scss]
    code-copy: true
    center-title-slide: false
    highlight-style: a11y
    code-link: true
    code-overflow: wrap
    height: 1080
    width: 1600
execute: 
  eval: true
  echo: true
  freeze: auto
---

<h1>Présentation de `TidyModels`</h1>

<h2>R addict</h2>

<hr>

<h3>Clément Rieux, Consultant data science Epsilon France chez EDF</h3>

<h3>22 Septembre 2022</h3>

<br>

<h3>

`r fontawesome::fa("github", "black")`   [github.com/clem_rxx/presentation](https://github.com/clem/pres)

![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/SVG/tidymodels.svg){.absolute top="425" left="1100" width="300"}

## Tidymodels

> **tidymodels** est une collection de packages de modélisation qui est semblable à tidyverse. Créé par l'auteur de **caret** : Max Kuhn.

. . .

```{r}
library(tidymodels) 

## ── Attaching packages ─────────────────────────── tidymodels ## 0.1.4 ──
## ✔ broom        0.7.11     ✔ recipes      0.1.17
## ✔ dials        0.0.10     ✔ rsample      0.1.1 
## ✔ dplyr        1.0.7      ✔ tibble       3.1.7 
## ✔ ggplot2      3.3.6      ✔ tidyr        1.1.4 
## ✔ infer        1.0.0      ✔ tune         0.1.6 
## ✔ modeldata    0.1.1      ✔ workflows    0.2.4 
## ✔ parsnip      0.1.7      ✔ workflowsets 0.1.0 
## ✔ purrr        0.3.4      ✔ yardstick    0.0.9 
## ── Conflicts ───────────────────────────────────────── ## tidymodels_conflicts() ──
## ✖ purrr::discard() masks scales::discard()
## ✖ dplyr::filter()  masks stats::filter()
## ✖ dplyr::lag()     masks stats::lag()
## ✖ recipes::step()  masks stats::step()
```

. . .

-   `rsample` : Data splitting et resampling
-   `broom` : Manipulation des données output
-   `recipes` : Préparation des données avant modélisation
-   `parsnip` : Collection de modèles
-   `yardstick` : Evaluation metrics
-   `dials/tune` : Tuning des paramètres
-   `worklows` : Création de workflow ...

## Objectifs

. . .

-   Encourager les bonnes méthodologies

. . .

-   Construire une structure stable

. . .

-   Permettre une grande variété de méthodologies

## Comment préparer les données avec tidymodels

step\_\*() <https://recipes.tidymodels.org/reference/>

<iframe src="https://recipes.tidymodels.org/reference/" width="100%" height="70%" frameBorder="0">

</iframe>

## Comment préparer les données avec tidymodels

-   `step_impute*()` Imputation des données
    -   `step_impute_mean()`
    -   `step_impute_linear()`
-   `step_log()`
-   `step_mutate()`
-   `step_sqrt()`
-   `step_cut()`
-   `step_dummy()`
-   `step_center()`
-   `step_normalize()`
-   `step_corr()`
-   `step_zv()`

## Comment choisir un modèle avec tidymodels

1.  Choisir un modèle
2.  Sélectionner le mode (Si nécessaire)
3.  Paramètrer le engine

## 1. Choisir un modèle

Pour avoir la liste des modèles disponibles : <https://www.tidymodels.org/find/parsnip/>

<iframe src="https://www.tidymodels.org/find/parsnip/" width="100%" height="70%" frameBorder="0">

</iframe>



## 1. Choisir un modèle

Régression linéaire

```{r}
#| eval: false
linear_reg(penalty = NULL, mixture = NULL)
```

. . .

Random Forest

```{r}
#| eval: false
rand_forest(mtry = NULL, trees = NULL, min_n = NULL)
```

## 2. Sélectionner le mode

```{r }
#| eval: false
linear_reg() %>% 
  set_mode(mode = "regression")
```

. . .

```{r}
#| eval: false
logistic_reg() %>% 
  set_mode(mode = "classification")
```

## 3. Paramètrer le engine

```{r}
#| eval: false
linear_reg() %>% 
  set_engine("spark" )
```

. . .

```{r}
#| eval: false
rand_forest() %>% 
  set_engine("randomForest")
```

. . .

```{r}
#| eval: false
rand_forest() %>% 
  set_engine("ranger")
```

## Exemple de classification

```{r}
#| echo: false
library(tidyverse)
data <- read_csv(file = here::here("data/aw_fb_data.csv"))
data = data[,-c(1:2)]
df = data %>% filter(device == 'apple watch')
# df_fb = data %>% filter(device == 'fitbit')
opts <- options(knitr.kable.NA = "")
```

::: {style="text-align: center"}
`Données`
:::

<br>

::: {style="font-size: 0.75em"}
```{r}
#| echo: false
knitr::kable(df %>% head(6))
```
:::

::: {style="text-align: center"}
<https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ZS2Z2J>
:::

## Exemple de classification

```{r}
set.seed(123)
data_split <- initial_split(df, 
                            strata = activity)

```

. . .

<br>

```{r}
data_split
```

<br>

```{r}
train_data <- training(data_split) 
test_data <- testing(data_split)
```

. . .

<br>

```{r}
nrow(train_data)
```

. . .

<br>

```{r}
nrow(test_data)
```

## Exemple de classification

```{r}
df_rec = recipe(activity ~ . , data = train_data) 
```

. . .

<br>

```{r}
df_rec
```

. . .

<br>

```{r}
summary(df_rec)
```

## Exemple de classification

```{r}
df_rec = recipe(activity ~ . , data = train_data) %>% 
  step_num2factor(gender,
                  transform = function(x) x + 1,
                  levels = c('femme', 'homme')) %>% 
  step_rm(device) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric()) %>% 
  step_zv(all_numeric()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.7) 
```

## Exemple de classification

```{r}
#| code-line-numbers: "2-9"
df_rec = recipe(activity ~ . , data = train_data) %>% 
  step_num2factor(gender,
                  transform = function(x) x + 1,
                  levels = c('femme', 'homme')) %>% 
  step_rm(device) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric()) %>% 
  step_zv(all_numeric()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.7) 
```

## Exemple de classification

```{r}
#| code-line-numbers: "2-4|5|6|7|8|9"
df_rec = recipe(activity ~ . , data = train_data) %>% 
  step_num2factor(gender,
                  transform = function(x) x + 1,
                  levels = c('femme', 'homme')) %>% 
  step_rm(device) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric()) %>% 
  step_zv(all_numeric()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.7) 
```

## Exemple de classification

```{r}
encoded = df_rec %>%
  prep() %>%
  juice()
```

. . .

<br>

```{r}
#| echo: false
knitr::kable(encoded %>% head(6))
```

## Exemple de classification

```{r}
rf_spec <- 
  rand_forest(trees = 200) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")
```

. . .

<br>

```{r}
activity_wflow <- 
  workflow() %>% 
  add_recipe(df_rec) %>% 
  add_model(rf_spec)
```

. . .

<br>

```{r}
activty_fit <- 
  activity_wflow %>% 
  fit(data = train_data)
```

## Exemple Classification

```{r}
predict(activty_fit, test_data)
```

. . .

<br>

```{r}
activity_pred <- 
  augment(activty_fit, test_data)
```

. . .

<br>

```{r}
table = activity_pred %>%
  select(activity, 
         contains(".pred_"))
```

. . .

::: {style="font-size: 0.45em"}
```{r}
#| echo: false
knitr::kable(table %>% head(6))
```
:::

## Exemple Classification

### Performances

```{r}
activity_pred %>% 
  select(-.pred_class) %>% 
  roc_auc(truth = as.factor(activity), contains(".pred_")) 
```

. . .

<br>

```{r}
activity_pred %>% 
  accuracy(truth = as.factor(activity), .pred_class)
```

## Exemple Classification

### Performances

```{r}
#| fig-align: "center"
activity_pred %>% 
  conf_mat(activity, .pred_class) %>% 
  autoplot(type = "heatmap")
```

## Exemple Classification

### Performances

```{r}
#| fig-align: "center"
activity_pred %>% 
  select(-.pred_class) %>% 
  roc_curve(truth = as.factor(activity), contains(".pred_")) %>% 
  autoplot()
```

## Exemple Classification

### Tuning parameter

```{r}
tune_spec <- rand_forest(
  mtry = tune(),
  trees = 200,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")
```

. . .

<br>

```{r}
tune_wf <- workflow() %>%
  add_recipe(df_rec) %>% 
  add_model(tune_spec)
```

. . .

<br>

```{r}
#| echo: false
doParallel::registerDoParallel()
```

```{r}
set.seed(100)
cv_folds <-
  vfold_cv(train_data, 
            v = 3, 
           strata = activity) 
```

## Exemple Classification

### Tuning parameter

```{r}
set.seed(345)
tune_res <- tune_grid(
  tune_wf,
  resamples = cv_folds,
  grid = 20
)

```

## Exemple Classification

### Tuning parameter

```{r}
#| fig-align: "center"
tune_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "ACC")
```

## Exemple Classification

### Tuning parameter

```{r}
rf_grid <- grid_regular(
  min_n(range = c(1, 8)),
  mtry(range = c(9, 20)),
  levels = 2 # nombre de niveaux par paramètre
)
```

. . .

<br>

```{r}
set.seed(456)
regular_res <- tune_grid(
  tune_wf,
  resamples = cv_folds,
  grid = rf_grid
)
```

## Exemple Classification

### Tuning parameter

```{r}
#| fig-align: "center"
regular_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "ACC")
```

## Exemple Classification

### Tuning parameter

```{r}
best_acc <- select_best(regular_res, "accuracy")

final_rf <- finalize_model(
  tune_spec,
  best_acc
)
```

## Exemple Classification

### Finalisation du workflow

```{r}
final_wf <- workflow() %>%
  add_recipe(df_rec) %>%
  add_model(final_rf)
```

. . .

<br>

```{r}
final_res <- final_wf %>%
  last_fit(data_split) # train sur train et evalue sur test
```

## Exemple Classification

### Performances

```{r}
final_res %>% 
  collect_metrics()
```

## Exemple Classification

### Performances

```{r}
#| fig-align: "center"
final_res %>% 
  collect_predictions() %>% 
  conf_mat(activity, .pred_class) %>% 
  autoplot(type = "heatmap")
```

## Exemple Classification

### Performances

```{r}
#| fig-align: "center"
final_res %>% 
  collect_predictions() %>% 
  select(-.pred_class) %>% 
  roc_curve(truth = as.factor(activity), contains(".pred_")) %>% 
  autoplot()
```

## Exemple Classification

### Enregistrement du modèle

```{r}

final_activity_model = final_res %>%
  extract_workflow()

saveRDS(final_activity_model, here::here("data", "activity_wf_model.rds"))
```


## Exemple de série temporelle

Pour les séries temporelles nous avons besoin de quelques library supplémentaires.

```{r}
library(modeltime)
library(timetk)
library(lubridate)
library(ggblanket)
```

:::: {.columns}

::: {.column width="45%"}

```{r}
#| echo: false
# import des données
ts_data <- read_csv("data/paris_temp.csv") %>% 
  select(date, temp)

# format prophet
ts_data$ds = ts_data$date
ts_data$y = ts_data$temp

ts_data <- ts_data %>% 
  select(-c(date, temp))


```

:::


::: {.column width="45%"}

```{r}
#| echo: false
knitr::kable(ts_data %>% tail(6))
```

:::

::::

## Exemple de série temporelle

La library `timetk` va nous servir à créer notre split train/test

```{r}
split = time_series_split(
  ts_data,
  assess = "15 days",
  cumulative = TRUE
)
```

. . .

```{r}
#| echo: false
#| #| #| fig-align: "center"
split %>% 
  tk_time_series_cv_plan() %>% 
  filter(ds > "2022-03-01") %>% 
  plot_time_series_cv_plan(ds, y)
```


## Exemple de série temporelle

Comme pour la classification nous pouvons créer une récipient pour créer des nouvelles features.
On pourrait également imaginer d'ajouter des régresseurs à notre jeu de données initial afin d'améliorer notre modèle.

```{r}
recipe_spec <- recipe(y ~ ds, training(split)) %>%
  step_timeseries_signature(ds) %>%
  step_rm(contains("am.pm"), contains("hour"), contains("minute"),
          contains("second"), contains("xts")) %>%
  step_fourier(ds, period = 365, K = 5) %>%
  step_dummy(all_nominal()) 
  
table = recipe_spec %>% prep() %>% juice()
```

. . .

```{r}
#| echo: false
knitr::kable(table %>% head(6))
```

## Exemple de série temporelle

Cependant pour les séries temporelles, une bonne pratique est de tester notre modèle sur plusieurs périodes différentes.

. . .

On va donc créer plusieurs splits de train/test

```{r}
ts_k_folds = rolling_origin(ts_data,
                            initial = 1600,
                            assess = 15, 
                            skip = 120)
```

. . .


```{r}
#| echo: false
#| #| fig-align: "center"
ts_k_folds  %>%  
  mutate(train = map(splits, analysis),
         test = map(splits, assessment)) %>%
  select(id, train, test) %>%
  pivot_longer(-id) %>%
  unnest(value) %>%
  filter(id %in% c("Slice01", "Slice10", "Slice24")) %>%
  group_by(id) %>% 
  slice(tail(row_number(), 100)) %>% 
  gg_line(x = ds,
          y = y,
          col = name,
          facet = id,
          facet_scales = 'free_x')
```

## Exemple de série temporelle

Création d'une fonction afin d'itérer un ou plusieurs modèles sur nos différents splits.


```{r}
#| code-line-numbers: "3-4|5|7-10|12-15|17-18|20-23|25-28|30-31|33-42"
tune_prophet = function(splits){
  
  train_data = analysis(splits)
  test_data = assessment(splits)
  splits_calib <- initial_time_split(train_data, prop = 0.85)
  
  model_spec_prophet_boost <- prophet_boost(
    prior_scale_changepoints = 0.01,
    prior_scale_seasonality = 5) %>%
    set_engine("prophet_xgboost") 
  
  workflow_fit_prophet_boost <- workflow() %>%
    add_recipe(recipe_spec) %>%
    add_model(model_spec_prophet_boost) %>%
    fit(training(splits_calib))
  
  calib_table = workflow_fit_prophet_boost %>%  
    modeltime_calibrate(testing(splits_calib))
  
  future_prophet_boost = calib_table %>% 
    modeltime_refit(train_data) %>% 
    modeltime_forecast(new_data = test_data,
                       actual_data = train_data)
  
  m_prophet = prophet::prophet(df = train_data,
                               seasonality.mode = 'additive',
                               changepoint.prior.scale = 0.01,
                               seasonality.prior.scale = 5)
  
  future = prophet::make_future_dataframe(m_prophet, periods = nrow(test_data),
                                          freq = 'day', include_history = FALSE)
  
  bind_rows(predict(m_prophet, future) %>% 
              select(ds, yhat) %>% 
              mutate(type = 'prophet'),
            
            future_prophet_boost %>% 
              filter(.model_desc != 'ACTUAL') %>% 
              select(ds = .index, yhat = .value) %>% 
              mutate(type = 'prophet_xgb')) %>% 
    
    left_join(test_data, by = 'ds')
  
}
```
## Exemple de série temporelle

On test nos modèles sur différentes périodes :

```{r}
ts_tune = ts_k_folds %>% mutate(result = map(splits, tune_prophet))
```

. . .

On peut donc comparer différents modèles :

:::: {.columns}

::: {.column width="45%"}

```{r}
#| echo: false
ts_tune %>% 
  select(id, result) %>% 
  unnest(result) %>% 
  group_by(id, type) %>% 
  arrange(ds) %>% 
  mutate(prev = paste0('prev_', row_number())) %>% 
  ungroup() %>% 
  select(prev, type, ds, yhat, y) %>% 
  group_by(prev, type) %>% 
  summarise(mae = mean(abs(y-yhat)),
            mape = mean(abs(y-yhat)/y)) %>% 
  ungroup() %>% 
  pivot_longer(cols=c(mae:mape), names_to="perf") %>% 
  pivot_wider(names_from = type, values_from = value) %>%
  filter(perf == 'mae') %>% 
  gg_point(x = prophet,
           y = prophet_xgb,
           title = 'MAE (Prophet vs Prophet XGB)') +
  geom_abline() 
```


:::


::: {.column width="45%"}

```{r}
#| echo: false
ts_tune %>% 
  select(id, result) %>% 
  unnest(result) %>% 
  group_by(id, type) %>% 
  arrange(ds) %>% 
  mutate(prev = paste0('prev_', row_number())) %>% 
  ungroup() %>% 
  mutate(mae = abs(y-yhat),
            mape = abs(y-yhat)/y,
         date = as.Date(ds)) %>% 
  gg_point(x = date,
          y = mae,
          y_breaks = scales::breaks_pretty(), 
          title = 'Ecart à la prévisions sur les différents périodes (Prophet vs Prophet XGB)',
          # y_labels = scales::label_percent(),
          facet = type,
          col = type)
```

:::

::::

## Exemple de série temporelle

Une fois qu'on a le modèle que l'on souhaite

```{r}
model_arima = arima_reg() %>% 
  set_engine("auto_arima") %>% 
  fit(y~ds, training(split))
```

. . .

<br>

```{r}
model_prophet  = prophet_reg(
  prior_scale_changepoints = 0.01,
  prior_scale_seasonality = 5) %>% 
  set_engine("prophet") %>% 
  fit(y~ds, training(split))
```

. . .

<br>

```{r}
model_prophet_xgb  = prophet_boost(prior_scale_changepoints = 0.01,
                                   prior_scale_seasonality = 5) %>%
  set_engine("prophet_xgboost") 
  
workflow_fit_prophet_boost <- workflow() %>%
    add_model(model_prophet_xgb) %>%
    add_recipe(recipe_spec) %>%
    fit(training(split))
```


## Exemple de série temporelle


# model
model_table = modeltime_table(
  model_arima,
  model_prophet,
  workflow_fit_prophet_boost
)

# calibrate
calib_table = model_table %>% 
  modeltime_calibrate(testing(split))

calib_table %>%  modeltime_accuracy()

# test
calib_table %>% 
  modeltime_forecast(
    new_data = testing(split),
    actual_data = ts_data,
    keep_data = T
  ) %>% 
  plot_modeltime_forecast()

#
future_forecast = calib_table %>% 
  modeltime_refit(ts_data) 

future_pred = future_forecast%>% 
  modeltime_forecast(
    h = '15 days',
    actual_data = ts_data,
    keep_data = T
  )

future_pred %>% plot_modeltime_forecast()




